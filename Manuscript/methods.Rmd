
# MATERIALS AND METHODS

## _Image acquisition_

<!--

Imaging with hyperpolarized 3He was performed under an Institutional Review Board 
(IRB)-approved protocol with written informed consent obtained from each subject. 
In addition, all imaging was performed under an Food and Drug Administration 
(FDA)-approved physician’s Investigational New Drug application (IND 57866) 
for hyperpolarized 3He. MRI data were acquired on a 1.5 T whole-body MRI scanner 
(Siemens Sonata, Siemens Medical Solutions, Malvern, PA) with broadband 
capabilities and a flexible 3He chest radiofrequency coil (RF; IGC Medical 
Advances, Milwaukee, WI; or Clinical MR Solutions, Brookfield, WI). During a 
10–20-second breath-hold following the inhalation of %300 mL of hyperpolarized 
3He mixed with %700 mL of nitrogen, a set of 19–28 contiguous axial sections 
were collected. Parameters of the fast low angle shot sequence for 3He MRI 
were as follows: repetition time msec / echo time msec, 7/3; flip angle
10$^\circ$; matrix, 80  128; field of view, 26  42 cm; section thickness, 10 mm; 
and intersection gap, none. The data were deidentified prior to analysis.

-->

MR imaging was performed using a 1.5T commercial scanner (Avanto, Siemens Medical 
Solutions, Malvern PA).   A 3-D gradient-echo based MR pulse sequence was used to 
acquire images covering the whole lung with isotropic resolution of 3.9 mm. 
Other parameters include  TR= 1.80 ms, TE= 0.78 ms, flip angle= 9 degree, 
bandwidth= 1090 Hz/Pixel. Total acquisition time varies between 5-8 seconds 
depending on the size of the subjects. 

## Preprocessing

\begin{figure}
\centering
\includegraphics[width=\textwidth]{./Figures/N4Denoised.pdf}
\caption{Sample image showing the effects of preprocessing on the proton MRI. (a) 
Uncorrected image showing MR field inhomogeneity and noise. (b) Corresponding corrected 
image in which the bias and noise effects have been ameliorated.}
\label{fig:n4denoised}
\end{figure}

## Template-based data augmentation

In addition to these software contributions, a significant methodological contribution 
we have made is the design of a template-based data augmentation strategy.  The need
for large training data sets is a well-known limitation associated with deep learning 
algorithms.  Whereas the architectures developed for such tasks as the ImageNet 
competition have access to millions of annotated images, such data access is not always
is available and such is typically the case in medical imaging.  In order to achieve
data set sizes necessary for learning functional models, various data augmentation 
strategies have been employed. These include application of intensity transformations, 
such as brightening and enhanced contrast, and simple spatial transformations, 
such as arbitrary rotations and translations.  Regarding the latter, such transformations
are not ideal as they might not reflect what is typically seen in medical images and
might not sufficiently sample the shape-space of the population currently being 
studied.  

We currently use a template-based approach whereby image data sampled from the population
is used to construct a representative template that is optimal in terms of both shape and 
intensity [@Avants:2010aa].  In addition to the representative template, this template-building 
process yields the transformations to/from each individual image to the template space.
This permits a propagation of the training data to the space of each individual image. In 
the simplest case, the training data is used to construct the template and then each 
individual training data is propagated to the space of every other individual training
data.  In this way, a training data set of size $N$ can be expanded to a data set of 
size $N^2$ (cf Figure 1).  A more complicated use case could build a template from $M$ data sets 
(where $M > N$).  Transformations between the training data and the template could then
be used to propagate the training data to the spaces of the individual members of the 
template-generating data for an augmented data set size of $M \times N$. 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{./Figures/DataAugmentation.pdf}
\caption{We introduce a novel data augmentation strategy for medical images using 
ANTs-based template construction.  Shown here is the 2-D U-net example where 
we create a template from the training data segmentation images where the 
foreground designates the left and right lungs.  This avoids the lack of 
internal correspondence while generating plausible global shape variations 
when mapping between individual training data.  We used 60+ images to 
create such a template permitting 60$^2$ = 3600 possible deformable shapes 
which can be further augmented by more conventional strategies (e.g., brightness
transformations, translations, etc.).}
\label{fig:augmentation}
\end{figure}

## ANTsRNet

The recent interest in deep learning techniques and the associated successes with respect to a 
variety of applications has motivated adoption of such techniques within the medical imaging 
research community.  Basic image operations such as classification, object identification, and 
segmentation (as well as more focused techniques) has significant potential for facilitating 
basic medical research.  In light of these new developments, and in order to better meet the 
modern needs of the community, we have modified this specific aim for ITK-Lung to include 
the implementation and dissemination of open-source deep learning architectures relevant to 
the use cases of our partner investigators.  

Towards this end, we have created _ANTsRNet_--a collection of well-known deep learning 
architectures ported to the R language.  ANTsRNet is built using the Keras neural network 
library (available through R) and is highly integrated with the ANTsR package, the R interface 
of the ANTs toolkit.  Consistent with our other software offerings, ongoing development is 
currently carried out on GitHub using a well-commented coding style, thorough documentation, 
and self-contained working examples.  

It should be noted that various implementations of different deep learning 
architectures exist and are largely available to the public.  However, we feel 
that this work fills an unmet need.  Based on our own search, many publicly 
available implementations, while functional, are not developed with large-scale distribution 
and application as end goals.  There is little, if any, coding consistency between the 
various implementations leading to non-standardized APIs and difficulties in code
navigation for debugging and/or didactic reasons.  In addition, the vast majority employ the
Python language which is understandable given its widespread usage by data scientists.
However, this work makes these powerful new developments available through a major platform 
heavily used by statisticians and data scientists alike.
In addition, the R-based interface to the ANTs toolkit allows for preprocessing and data
augmentation strategies specific to medical imaging.  As a result of these current efforts,
we were recently awarded a Titan XP GPU from the NVIDIA corporation for facilitating ongoing 
development.

Although much work remains to be completed, we have made significant progress. As noted below,
several architectures have been implemented for both 2-D and 3-D images spanning the broad
application areas of image classification, object detection, and image segmentation. 
It should be noted that most reporting in the literature has dealt exclusively with 2-D 
implementations.  This is understandable due to memory and computational speed constraints
limiting practical 3-D application on current hardware.  However, given the importance that
3-D data has for medical imaging and the rapid progress in hardware, we  feel it worth
the investment in implementing corresponding 3-D architectures.  Each 
architecture is accompanied by one or more self-contained examples for testing and illustrative
purposes.  In addtion, we have made novel data augmentation strategies available to the user 
and illustrated them with Keras-specific batch generators.    These contributions are outlined 
below.  

\begin{table}[!htb]
\centering
\caption{My caption}
\label{table:antsrnet}
\begin{tabular*}{0.9\textwidth}{ll@{\extracolsep{\fill}}l}
\toprule
\multicolumn{3}{c}{\textbf{ANTsRNet}}    \\        
\midrule
\multicolumn{3}{l}{\textbf{Image Segmentation}}   \\
U-net \cite{Ronneberger:2015} &     (2-D, 3-D)       & skdfjsk               \\
V-net \cite{Milletari:2016} &     (2-D, 3-D)       & skdfjsk               \\
\midrule
\multicolumn{3}{l}{\textbf{Image Classification}}   \\
AlexNet \cite{Krizhevsky:2012} & (2-D, 3-D)            & skdfjsk               \\
Vgg16/Vgg19 \cite{Simonyan:2014} & (2-D, 3-D)        & blah-blah               \\
GoogLeNet \cite{Szegedy:2015} & (2-D only) & \\
ResNet \cite{He:2015}  & (2-D, 3-D)            & skdfjsk               \\
ResNeXt \cite{Xie:2016} & (2-D, 3-D)            & skdfjsk               \\
DenseNet \cite{Huang:2016} & (2-D, 3-D) & \\
\midrule
\multicolumn{3}{l}{\textbf{Object Localization}}   \\
SSD300/SSD512 \cite{Liu:2015} & (2-D, 3-D)      & skdfjsk               \\
SSD7  & (2-D, 3-D)             & blah-blah               \\
\bottomrule
\end{tabular*}
\end{table}


-------------------------------------------------------------
                    __ANTsRNet__
-------------------------------------------------------------
__Image Segmentation__

------------------------------ -------------- ---------------
U-net [@Ronneberger:2015]      (2-D, 3-D)     blah-blah

V-net [@Milletari:2016]        (2-D, 3-D)     blah-blah
-------------------------------------------------------------

Table: Here's the caption. It, too, may span
multiple lines.

<!--
 
__Image classification__

* __AlexNet.__  Although convolutional neural networks (CNNs) have been around since the 
1970s, it was the ImageNet competition of 2012 and the superior results produced by the AlexNet 
architecture [@Krizhevsky:2012] that spurred its subsequent popularity such that CNNs are now 
the preferred approach to image-based neural networks.  Although originally only 2-D, both 
2-D and 3-D implementations have been implemented.  Example test code employs the MNIST
data set for classifying handwritten digits directly downloadable within R.

* __Vgg16/Vgg19.__ OxfordNet, or VGG, architectures [@Simonyan:2014] are much deeper than AlexNet and featured
well in the 2014 ImageNet challenge.  We implemented popular 16- and 19-layer versions for 
ANTsRNet.  Given the simplicity and excellent performance, these form the classification component
of such object detection architectures as the multibox Single-Shot Detection (SSD) network 
described below.  Both 2-D and 3-D versions have been implemented.   Example test code 
employs the MNIST data set.

* __GoogLeNet.__  GoogLeNet, or Inception (version v3) [@Szegedy:2015], is a 22-layer network 
characterized by _inception blocks_ meant to reduce the number of parameters necessary to learn 
the targeted function.  The architecture prevents a straightforward 3-D implementation so only
a 2-D architecture is currently available.  Example test code employs the MNIST data set.

* __ResNet/ResNeXt.__  The original ResNet architecture [@He:2015], along with a variant known 
as _ResNeXt_[@Xie:2016], is also included in ANTsRNet.  ResNet, characterized by specialized blocks and
skip connections, won the ImageNet challenge in 2015.  Both 2-D and 3-D versions have been 
implemented.   Example test code employs the MNIST data set.

*  __DenseNet.__  The DenseNet architecture [@Huang:2016] is based on the observation that performance is
typically enhanced with shorter connections between the layers and the input.  This leads to
an architecture in which every layer is connected to every other layer substantially reducing
the number of parameters as well as other benefits.  Both 2-D and 3-D versions have been 
implemented.

__Object detection__

* __SSD7/SSD300/SSD512.__ A common preprocssing step in many medical imaging tsks is the localization
of an object or region of interest.  The Multibox Single-Shot Detection (SSD) algorithm is a 
well-known architecture with good performance [@Liu:2015]. We have implemented the original 2-D '300'- and 
'512'-style SSD networks in addition to their 3-D extensions.  As these networks require significant
training for determining optimal weighting, we also implemented a smaller architecture known as SSD7
which does not have such training data requirements.  We also extended this architecture to 3-D.
A self-contained 2-D example of labeled faces 
demonstrates training and testing of the SSD7 architecture.  

__Image segmentation__

* __U-Net/V-net.__  Extending fully convolutional neural networks (fCNN) by including an upsampling
decoding path with skip connections linking corresponding encoding/decoding layers, the authors of
U-net [@Ronneberger:2015] created a well-performing deep learning segmentation framework for 2-D images.
This was later extended to 3-D with a custom Dice loss function in [@Milletari:2016] denoted as
V-net.  Both 2-D and 3-D versions are implemented with a custom loss Dice function based on our 
work in the Insight Toolkit [@tustison2009].  We have also created specialized decoding and encoding 
utilities for translating between ANTs images and data representations necessary for Keras operations.
Examples include a left/right lung segmentation example which includes a demonstration of our 
unique template-based data augmentation strategy (see below).

-->
