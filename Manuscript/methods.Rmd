
# MATERIALS AND METHODS

## _Image acquisition_

Both proton and ventilation mages used for this study were taken from current and
previous studies from our group. Ventilation images comprised both Helium-3 and 
Xenon-129 acquisitions as our current segmentation processing does not distinguish 
between acquisition protocols and we expected similar agnosticism for the proposed
approach.

__Ventilation.__
Hyperpolarized MR image acquisition was performed under an Institutional Review Board 
(IRB)-approved protocol with written informed consent obtained from each subject. 
In addition, all imaging was performed under an Food and Drug Administration 
(FDA)-approved physician’s Investigational New Drug application (IND 57866) 
for hyperpolarized 3He. MRI data were acquired on a 1.5 T whole-body MRI scanner 
(Siemens Sonata, Siemens Medical Solutions, Malvern, PA) with broadband 
capabilities and a flexible 3He chest radiofrequency coil (RF; IGC Medical 
Advances, Milwaukee, WI; or Clinical MR Solutions, Brookfield, WI). During a 
10–20-second breath-hold following the inhalation of hyperpolarized gas, a set 
of 19–28 contiguous axial sections were collected. Parameters of the fast low angle shot sequence 
were as follows: repetition time msec / echo time msec, 7/3; flip angle
10$^\circ$; matrix, 80 $\times$ 128; field of view, 26 $\times$ 42 cm; section thickness, 10 mm;
and intersection gap, none. 
Total acquisition time varies between 5-8 seconds depending on the size of the subjects. 

__Proton.__
A three-dimensional (3D) proton gradient-echo sequence 
(repetition time [TR]:1.80 ms, echo time [TE] 0.78 ms, flip angle 10$^\circ$, bandwidth per 
pixel 1090 Hz/Pixel, partial Fourier: phase direction 6/8, slice direction 6/8) 
was used to acquire multiple images sets from multiple subjects at varying inflation levels.
Acquisition time was 4 sec per image set.  All imaging studies were performed under a 
physician’s Investigational New Drug application for Xe129 MRI using a protocol approved 
by Institutional Review Board of our institute. All subjects provided written informed consent
and the data were deidentified prior to analysis.

<!-- 

MR imaging was performed using a 1.5T commercial scanner (Avanto, Siemens Medical 
Solutions, Malvern PA).   A 3-D gradient-echo based MR pulse sequence was used to 
acquire images covering the whole lung with isotropic resolution of 3.9 mm. 
Other parameters include  TR = 1.80 ms, TE = 0.78 ms, flip angle = 9 degree, 
bandwidth= 1090 Hz/Pixel. Total acquisition time varies between 5-8 seconds 
depending on the size of the subjects. 

-->

## _Image processing and analysis_

We first review our previous contributions to the segmentation of proton and 
hyperpolarized gas MR images [@Tustison:2011aa;@Tustison:2016aa] as we use 
these previously described techniques for evaluative comparison.  We then describe 
the deep learning analogs (including preprocessing) extending earlier work
and discuss the proposed comntributions which include:

* convolution neural networks for structural/functional lung
segmentation,

* template-based data augmentation, and

* open-source (i.e., ANTsRNet) availability.


### Previous approaches for lung and ventilation-based segmentation

Automated ventilation-based segmentation, described in [@Tustison:2011aa], employs
a Gaussian mixture model using a Markov random field (MRF) spatial prior which is optimized
via the expectation-maximization algorithm which has been used in a number of 
clinical studies (e.g., [@Altes:2016aa;@Altes:2017aa]).  Briefly, the intensity histogram profile
is modeled using Gaussians with means and standard deviations designed to 
model the intensities of the individual ventilation classes 
(e.g., ventilation defect, hypo-ventilation, and normal ventilation).  At each iteration
the resulting estimated labels are then refined based an MRF spatial modeling to smooth
out the effects of noise.  The parameters of the class-specific Gaussians are then re-estimated.  This iterative
process continues until convergence.  We also iterate this segmentation with application
of N4 bias correction [@Tustison:2010ac].  Unlike other segmentation methods which rely 
solely on intensity distributions while discarding spatial information,
(e.g., K-means variants [@Kirby:2012aa;@Zha:2016aa], 
histogram rescaling and thresholding [@He:2014aa]),  our technique employs
both spatial and intensity information for probabilistic classification.

Because of our dual structural/functional acquisition protocol [@], we also 
previously formulated a joint label fusion (JLF) approach for segmenting
the left and right lungs in proton MRI as well as estimating the lobar volumes.  An 
atlas set consisting of a cohort of both 
the proton MRI and the corresponding lung segmentation is spatially normalized to an 
unlabeled image. A weighted consensus from the normalized images and segmentations 
is used to determine each voxel label.  Although the method yields high quality 
results which are fully automated, one of the drawbacks is the time and computational 
resources required to perform the image registration for each member of the atlas set 
and the subsequent voxelwise label estimation.   

### Preprocessing

Because of the low-frequency artifacts introduced by such confounds as 
radiofrequency coil inhomogeneity, we perform a retrospective bias 
correction on both sets of images using the N4 algorithm [@tustison2010].
These are included in our previously proposed ventilation [@Tustison:2011aa]
and structural [@Tustison:2016aa] segmentation frameworks.  Since the initial 
release of these pipelines we have also adopted an adaptive, patch-based
denoising algorithm specific to MR [@Manjon:2010aa] which we have reimplemented 
in the ANTs toolkit.  The dual effects of these data cleaning techniques on both 
the proton images and ventilation images are shown in Figure
\ref{fig:n4denoised}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{./Figures/N4Denoised.pdf}
\caption{Side-by-side image comparison showing the effects of preprocessing on the proton (top) 
and ventilation (bottom) MRI. (a) 
Uncorrected image showing MR field inhomogeneity and noise. (b) Corresponding corrected 
image in which the bias and noise effects have been ameliorated.}
\label{fig:n4denoised}
\end{figure}


### U-net architecture

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{./Figures/Unet.png}
\caption{Illustration
}
\label{fig:unet}
\end{figure}






A diagrammatic representation of the proposed workflow is provided in Figure 
\ref{fig:workflow}.


### Template-based data augmentation

In addition to these software contributions, a significant methodological contribution 
we have made is the design of a template-based data augmentation strategy.  The need
for large training data sets is a well-known limitation associated with deep learning 
algorithms.  Whereas the architectures developed for such tasks as the ImageNet 
competition have access to millions of annotated images, such data access is not always
is available and such is typically the case in medical imaging.  In order to achieve
data set sizes necessary for learning functional models, various data augmentation 
strategies have been employed. These include application of intensity transformations, 
such as brightening and enhanced contrast, and simple spatial transformations, 
such as arbitrary rotations and translations.  Regarding the latter, such transformations
are not ideal as they might not reflect what is typically seen in medical images and
might not sufficiently sample the shape-space of the population currently being 
studied.  

We currently use a template-based approach whereby image data sampled from the population
is used to construct a representative template that is optimal in terms of both shape and 
intensity [@Avants:2010aa].  In addition to the representative template, this template-building 
process yields the transformations to/from each individual image to the template space.
This permits a propagation of the training data to the space of each individual image. In 
the simplest case, the training data is used to construct the template and then each 
individual training data is propagated to the space of every other individual training
data.  In this way, a training data set of size $N$ can be expanded to a data set of 
size $N^2$ (cf Figure 1).  A more complicated use case could build a template from $M$ data sets 
(where $M > N$).  Transformations between the training data and the template could then
be used to propagate the training data to the spaces of the individual members of the 
template-generating data for an augmented data set size of $M \times N$. 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{./Figures/DataAugmentation.pdf}
\caption{We introduce a novel data augmentation strategy for medical images using 
ANTs-based template construction.  Shown here is the 2-D U-net example where 
we create a template from the training data segmentation images where the 
foreground designates the left and right lungs.  This avoids the lack of 
internal correspondence while generating plausible global shape variations 
when mapping between individual training data.  We used 60+ images to 
create such a template permitting 60$^2$ = 3600 possible deformable shapes 
which can be further augmented by more conventional strategies (e.g., brightness
transformations, translations, etc.).}
\label{fig:augmentation}
\end{figure}



\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{./Figures/workflow.pdf}
\caption{Illustration of the proposed workflow.  Training the U-net models for both proton and 
ventilation imaging includes template-based data augmentation.  This offline training is computationally 
intensive but is only performed once.   Subsequent individual subject preprocessing includes
MR denoising and bias correction.  The proton mask determined from the proton U-net model 
is included as a separate "channel" for ventilation image processing.s
}
\label{fig:workflow}
\end{figure}

### ANTsRNet

The recent interest in deep learning techniques and the associated successes with respect to a 
variety of applications has motivated adoption of such techniques within the medical imaging 
research community.  Basic image operations such as classification, object identification, and 
segmentation (as well as more focused techniques) has significant potential for facilitating 
basic medical research.  In light of these new developments, and in order to better meet the 
modern needs of the community, we have modified this specific aim for ITK-Lung to include 
the implementation and dissemination of open-source deep learning architectures relevant to 
the use cases of our partner investigators.  

Towards this end, we have created _ANTsRNet_--a collection of well-known deep learning 
architectures ported to the R language.  ANTsRNet is built using the Keras neural network 
library (available through R) and is highly integrated with the ANTsR package, the R interface 
of the ANTs toolkit.  Consistent with our other software offerings, ongoing development is 
currently carried out on GitHub using a well-commented coding style, thorough documentation, 
and self-contained working examples.  

It should be noted that various implementations of different deep learning 
architectures exist and are largely available to the public.  However, we feel 
that this work fills an unmet need.  Based on our own search, many publicly 
available implementations, while functional, are not developed with large-scale distribution 
and application as end goals.  There is little, if any, coding consistency between the 
various implementations leading to non-standardized APIs and difficulties in code
navigation for debugging and/or didactic reasons.  In addition, the vast majority employ the
Python language which is understandable given its widespread usage by data scientists.
However, this work makes these powerful new developments available through a major platform 
heavily used by statisticians and data scientists alike.
In addition, the R-based interface to the ANTs toolkit allows for preprocessing and data
augmentation strategies specific to medical imaging.  As a result of these current efforts,
we were recently awarded a Titan XP GPU from the NVIDIA corporation for facilitating ongoing 
development.

Although much work remains to be completed, we have made significant progress. As noted below,
several architectures have been implemented for both 2-D and 3-D images spanning the broad
application areas of image classification, object detection, and image segmentation. 
It should be noted that most reporting in the literature has dealt exclusively with 2-D 
implementations.  This is understandable due to memory and computational speed constraints
limiting practical 3-D application on current hardware.  However, given the importance that
3-D data has for medical imaging and the rapid progress in hardware, we  feel it worth
the investment in implementing corresponding 3-D architectures.  Each 
architecture is accompanied by one or more self-contained examples for testing and illustrative
purposes.  In addtion, we have made novel data augmentation strategies available to the user 
and illustrated them with Keras-specific batch generators.    These contributions are outlined 
below.  

\input{antsrnetTable.tex}

---
nocite: |
  @Ronneberger:2015, @Milletari:2016, @Krizhevsky:2012, @Simonyan:2014, @Szegedy:2015, @He:2015, @Xie:2016, @Huang:2016, @Liu:2015, @ssd7}
...
