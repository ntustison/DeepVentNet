
# MATERIALS AND METHODS

## _Image acquisition_

Both proton and ventilation mages used for this study were taken from current and
previous studies from our group. Ventilation images comprised both Helium-3 and 
Xenon-129 acquisitions as our current segmentation processing does not distinguish 
between acquisition protocols and we expected similar agnosticism for the proposed
approach.

__Ventilation.__
Hyperpolarized MR image acquisition was performed under an Institutional Review Board 
(IRB)-approved protocol with written informed consent obtained from each subject. 
In addition, all imaging was performed under an Food and Drug Administration 
(FDA)-approved physician’s Investigational New Drug application (IND 57866) 
for hyperpolarized 3He. MRI data were acquired on a 1.5 T whole-body MRI scanner 
(Siemens Sonata, Siemens Medical Solutions, Malvern, PA) with broadband 
capabilities and a flexible 3He chest radiofrequency coil (RF; IGC Medical 
Advances, Milwaukee, WI; or Clinical MR Solutions, Brookfield, WI). During a 
10–20-second breath-hold following the inhalation of hyperpolarized gas, a set 
of 19–28 contiguous axial sections were collected. Parameters of the fast low angle shot sequence 
were as follows: repetition time msec / echo time msec, 7/3; flip angle
10$^\circ$; matrix, 80 $\times$ 128; field of view, 26 $\times$ 42 cm; section thickness, 10 mm;
and intersection gap, none. 
Total acquisition time varies between 5-8 seconds depending on the size of the subjects. 

__Proton.__
A three-dimensional (3D) proton gradient-echo sequence 
(repetition time [TR]:1.80 ms, echo time [TE] 0.78 ms, flip angle 10$^\circ$, bandwidth per 
pixel 1090 Hz/Pixel, partial Fourier: phase direction 6/8, slice direction 6/8) 
was used to acquire multiple images sets from multiple subjects at varying inflation levels.
Acquisition time was 4 sec per image set.  All imaging studies were performed under a 
physician’s Investigational New Drug application for Xe129 MRI using a protocol approved 
by Institutional Review Board of our institute. All subjects provided written informed consent
and the data were deidentified prior to analysis.

<!-- 

MR imaging was performed using a 1.5T commercial scanner (Avanto, Siemens Medical 
Solutions, Malvern PA).   A 3-D gradient-echo based MR pulse sequence was used to 
acquire images covering the whole lung with isotropic resolution of 3.9 mm. 
Other parameters include  TR = 1.80 ms, TE = 0.78 ms, flip angle = 9 degree, 
bandwidth= 1090 Hz/Pixel. Total acquisition time varies between 5-8 seconds 
depending on the size of the subjects. 

-->

## _Image processing and analysis_

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{./Figures/workflow.pdf}
\caption{Illustration of the proposed workflow.  Training the U-net models for both proton and 
ventilation imaging includes template-based data augmentation.  This offline training is computationally 
intensive but is only performed once.   Subsequent individual subject preprocessing includes
MR denoising and bias correction.  The proton mask determined from the proton U-net model 
is included as a separate "channel" for ventilation image processing.s
}
\label{fig:workflow}
\end{figure}

We first review our previous contributions to the segmentation of proton and 
hyperpolarized gas MR images [@Tustison:2011aa;@Tustison:2016aa] as we use 
these previously described techniques for evaluative comparison.  We then describe 
the deep learning analogs (including preprocessing) extending earlier work
and discuss the proposed contributions which include:

* convolution neural networks for structural/functional lung
segmentation,

* template-based data augmentation, and

* open-source availability.

An overview of the resulting framework is provided in Figure  \ref{fig:workflow}.
The most computationally intensive portion is the offline processing for model
training for both structural and functional imaging.  However, once that is 
complete, individual processing consists of a couple of preprocessing steps
followed by application of the models which has minimal computational requirements.  

### Previous approaches for lung and ventilation-based segmentation

Automated ventilation-based segmentation, described in [@Tustison:2011aa], employs
a Gaussian mixture model using a Markov random field (MRF) spatial prior which is optimized
via the expectation-maximization algorithm which has been used in a number of 
clinical studies (e.g., [@Altes:2016aa;@Altes:2017aa]).  Briefly, the intensity histogram profile
is modeled using Gaussians with means and standard deviations designed to 
model the intensities of the individual ventilation classes 
(e.g., ventilation defect, hypo-ventilation, and normal ventilation).  At each iteration
the resulting estimated labels are then refined based an MRF spatial modeling to smooth
out the effects of noise.  The parameters of the class-specific Gaussians are then re-estimated.  This iterative
process continues until convergence.  We also iterate this segmentation with application
of N4 bias correction [@Tustison:2010ac].  Unlike other segmentation methods which rely 
solely on intensity distributions while discarding spatial information,
(e.g., K-means variants [@Kirby:2012aa;@Zha:2016aa], 
histogram rescaling and thresholding [@He:2014aa]),  our technique employs
both spatial and intensity information for probabilistic classification.

Because of our dual structural/functional acquisition protocol [@Qing:2015aa], we also 
previously formulated a joint label fusion (JLF) approach for segmenting
the left and right lungs in proton MRI as well as estimating the lobar volumes.  An 
atlas set consisting of a cohort of both 
the proton MRI and the corresponding lung segmentation is spatially normalized to an 
unlabeled image. A weighted consensus from the normalized images and segmentations 
is used to determine each voxel label.  Although the method yields high quality 
results which are fully automated, one of the drawbacks is the time and computational 
resources required to perform the image registration for each member of the atlas set 
and the subsequent voxelwise label estimation.   

### Preprocessing

Because of the low-frequency imaging artifacts introduced by confounds such as 
radiofrequency coil inhomogeneity, we perform a retrospective bias 
correction on both sets of images using the N4 algorithm [@tustison2010].
These are included in our previously proposed ventilation [@Tustison:2011aa]
and structural [@Tustison:2016aa] segmentation frameworks.  Since the initial 
release of these pipelines we have also adopted an adaptive, patch-based
denoising algorithm specific to MR [@Manjon:2010aa] which we have reimplemented 
in the ANTs toolkit.  The dual effects of these data cleaning techniques on both 
the proton images and ventilation images are shown in Figure
\ref{fig:n4denoised}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{./Figures/N4Denoised.pdf}
\caption{Side-by-side image comparison showing the effects of preprocessing on the proton (top) 
and ventilation (bottom) MRI. (a) 
Uncorrected image showing MR field inhomogeneity and noise. (b) Corresponding corrected 
image in which the bias and noise effects have been ameliorated.}
\label{fig:n4denoised}
\end{figure}


### U-net architecture for structural/functional lung segmentation

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{./Figures/Unet.png}
\caption{The modified U-net architecture for both structural and functional 
lung segmentation.  Network layers are represented as boxes with arrows 
designating connections between layers.  The main parameter value for each layer
is provided above the corresponding box.  Input 2-D images are typically of
size $128 \times 128$ voxels or are resampled accordingly.  Each layer 
of the descending (or "encoding") branch of the network is characterized
by two convolutional layers.  Modification of 
the original architecture includes an intermediate dropout layer 
for regularization (dropout rate = 0.2).  A max pooling operation 
 produces the feature map for the next series.  The ascending
(or "decoding") branch is similarly characterized.  A convolutional transpose
operation is used to upsample the feature map following a convolution $\rightarrow$
dropout $\rightarrow$ convolution layer series until the final convolutional
operation which yields the segmentation probability maps.
}
\label{fig:unet}
\end{figure}


Deep learning, a term denoting neural network architectures with 
multiple hidden layers, has gained prominence in recent years.  In the field of 
image analysis and computer vision, deep learning with convolution neural networks (CNNs)
has been particularly prominent due, in large part, to the annual ImageNet Large Scale Visual 
Recognition Challenge [@Russakovsky:2015aa].
Specifically, one of the participating groups in the 2012 ImageNet challenge was the earliest 
adopter of CNNs.  The resulting architecture, colloquially known as "AlexNet" [@AlexNet],
surpassed any approach that had been proposed previously and laid the groundwork for
future CNN-based architectures for image classification such as VGG [@Simonyan:2014] and
GoogLeNet [@Szegedy:2015].  The recent successes of CNNs are historically rooted in the 
pioneering work of LeCun et al. [@LeCun1998] and Fukushima [@Fukushima:1980aa] and others
which drew inspiration from earlier work on the complex arrangment of cells within 
the feline visual cortex [@HUBEL:1962aa].  CNNs are characterized by common components i.e.,
convolution, pooling, and activation functions which can be put together in various 
arrangements to perform such tasks as image classification and voxelwise segmentation.

The U-net architecture was introduced in [@Ronneberger:2015aa] which extended the fully 
convolutional neural network (FCN) approach introduced by Long, Shelhamer, and Darrel 
[@Shelhamer:2017aa].  U-net augments the "encoding path" (see left side of Figure 
\ref{fig:unet}) common to such architectures as VGG and FCN with a symmetric decoding
path where the corresponding encoding/decoding layers are linked via skip paths for
enhanced feature detection.  The nomenclature reflects the descending/ascending aspect
of its architecture.  Each series in both encoding and decoding branches is 
characterized by two convolutional layers sandwiching an optional dropout layer.  This 
latter modification from the original is meant to provide additional regularization for 
over-fitting prevention [@Srivastava2014].  Output consists of a segmentation probability
image for each label from which a segmentation map can be inferred.  

We used U-net to build separate models for segmenting both structural and functional lung 
images.  For cases where dual acquisition provides both images, we use the structural 
images to provide a mask for segmentation of the ventilation image.  We used an open-source
implementation written by our group and provided with the ANTsRNet R package [@antsrnet] which
is described in greater detail below.  We also implemented a multi-label dice coefficient
loss function along with specific batch generators for generating batch image data on the
fly.  Specific parameters for the architectural components
are as follows:

* Convolution layers
    * kernel size:  $2 \times 2$
    * activation: rectified linear units (ReLU) [@Nair2010]
    * number of filters:  doubled at every layer starting with $N = 32$
* Dropout layers
    * rate: 0.2
* Max pooling layers
    * size: $2 \times 2$
    * stride length:  $2 \times 2$
* Upsampling or deconvolution layers
    * kernel size:  $3 \times 3$
    * stride length:  $2 \times 2$
    * activation: rectified linear units (ReLU) [@Nair2010]
    * number of filters:  doubled at every layer starting with $N = 32$



### Template-based data augmentation

In order to generate data cohorts of sufficient size necessary for deep learning
approaches, we have designed of a template-based data augmentation strategy.  The need
for large training data sets is a well-known limitation associated with deep learning 
algorithms.  Whereas the architectures developed for such tasks as the ImageNet 
competition have access to millions of annotated images, such data access is not always
available and such is typically the case in medical imaging.  In order to achieve
data set sizes necessary for learning functional models, various data augmentation 
strategies have been employed [@Taylor:2017aa]. These include application of intensity 
transformations, such as brightening and enhanced contrast.  They might also include 
spatial transformations such as arbitrary rotations, translations, and even simulated
elastic deformations.  Such transformations might not be ideal if they do not represent
shape variation within the range within the population under study.

We propose a template-based approach whereby image data sampled from the population
is used to construct a representative template that is optimal in terms of shape and/or
intensity [@Avants:2010aa].  In addition to the representative template, this template-building 
process yields the transformations to/from each individual image to the template space.
This permits a propagation of the training data to the space of each individual image. In 
the simplest case, the training data is used to construct the template and then each 
individual training data is propagated to the space of every other individual training
data.  In this way, a training data set of size $N$ can be expanded to a data set of 
size $N^2$ (cf Figure 1).  A more complicated use case could build a template from $M$ data sets 
(where $M > N$).  Transformations between the training data and the template could then
be used to propagate the training data to the spaces of the individual members of the 
template-generating data for an augmented data set size of $M \times N$. 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{./Figures/DataAugmentation.pdf}
\caption{We introduce a novel data augmentation strategy for medical images using 
ANTs-based template construction.  Shown here is the 2-D U-net example where 
we create a template from the training data segmentation images where the 
foreground designates the left and right lungs.  This avoids the lack of 
internal correspondence while generating plausible global shape variations 
when mapping between individual training data.  We used 60+ images to 
create such a template permitting 60$^2$ = 3600 possible deformable shapes 
which can be further augmented by more conventional strategies (e.g., brightness
transformations, translations, etc.).}
\label{fig:augmentation}
\end{figure}

### ANTsRNet

In addition to the contributions previously described, we also introduce ANTsRNet [@antsrnet] to the 
research community which not only contains the software to perform the operations specific 
to structural and functional lung image segmentation but also performs a host of other 
deep learning tasks wrapped in a thoroughly documented and well-written R package.
The recent interest in deep learning techniques and the associated successes with respect to a 
variety of applications has motivated adoption of such techniques.
Basic image operations such as classification, object identification, and 
segmentation (as well as more focused techniques) have significant potential for facilitating 
basic medical research.  ANTsRNet is built using the Keras neural network 
library (available through R) and is highly integrated with the ANTsR package, the R interface 
of the ANTs toolkit.  Consistent with our other software offerings, ongoing development is 
currently carried out on GitHub using a well-commented coding style, thorough documentation, 
and self-contained working examples.  

It should be noted that various implementations of different deep learning 
architectures exist and are largely available to the public.  However, we feel 
that ANTsRNet fills an unmet need.  Based on our own search, many publicly 
available implementations, while functional, are not developed with large-scale distribution 
and application as end goals.  There is little, if any, coding consistency between the 
various implementations leading to non-standardized APIs and difficulties in code
navigation for debugging and/or didactic reasons.  In addition, the vast majority employ the
Python language which is understandable given its widespread usage by data scientists.
However, this work makes these powerful new developments available through a major platform 
heavily used by statisticians and data scientists.
In addition, the R-based interface to the ANTs toolkit allows for preprocessing and data
augmentation strategies specific to medical imaging.  

Several architectures have been implemented for both 2-D and 3-D images spanning the broad
application areas of image classification, object detection, and image segmentation (cf. Table \ref{table:antsrnet}). 
It should be noted that most reporting in the literature has dealt exclusively with 2-D 
implementations.  This is understandable due to memory and computational speed constraints
limiting practical 3-D application on current hardware.  However, given the importance that
3-D data has for medical imaging and the rapid progress in hardware, we  feel it worth
the investment in implementing corresponding 3-D architectures.  Each 
architecture is accompanied by one or more self-contained examples for testing and illustrative
purposes.  In addtion, we have made novel data augmentation strategies available to the user 
and illustrated them with Keras-specific batch generators.    

\input{antsrnetTable.tex}

---
nocite: |
  @Ronneberger:2015, @Milletari:2016, @Krizhevsky:2012, @Simonyan:2014, @Szegedy:2015, @He:2015, @Xie:2016, @Huang:2016, @Liu:2015, @ssd7}
...
