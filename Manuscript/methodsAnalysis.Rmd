
## _Image processing and analysis_

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{./Figures/workflow.pdf}
\caption{Illustration of the proposed workflow.  Training the U-net models for both proton and 
ventilation imaging includes template-based data augmentation.  This offline training is computationally 
intensive but is only performed once.   Subsequent individual subject preprocessing includes
MR denoising and bias correction.  The proton mask determined from the proton U-net model 
is included as a separate channel (in deep learning software parlance) for ventilation image processing.
}
\label{fig:workflow}
\end{figure}

We first review our previous contributions to the segmentation of proton and 
hyperpolarized gas MR images [@Tustison:2011aa;@Tustison:2016aa] as we use 
these previously described techniques for evaluative comparison.  We then describe 
the deep learning analogs (including preprocessing) extending earlier work
and discuss the proposed contributions which include:

* convolution neural networks for structural/functional lung
segmentation,

* template-based data augmentation, and

* open-source availability.

An overview of the resulting framework is provided in Figure \ref{fig:workflow}.
The most computationally intensive portion is the offline processing for model
training for both structural and functional imaging.  However, once that is 
complete, individual processing consists of a couple of preprocessing steps
followed by application of the models which has minimal computational requirements.  

### Previous approaches from our group for lung and ventilation-based segmentation

The automated ventilation-based segmentation, described in [@Tustison:2011aa], employs
a Gaussian mixture model with a Markov random field (MRF) spatial prior optimized
via the Expectation-Maximization algorithm.  The resulting software, called 
Atropos, has been used in a number of 
clinical studies (e.g., [@Altes:2016aa;@Altes:2017aa]).  Briefly, the intensity histogram profile
of the ventilation image is modeled using Gaussian functions with optimizable parameters 
(i.e., mean, standard deviation, and normalization factor) designed to model the intensities 
of the individual ventilation classes.  At each iteration
the resulting estimated voxelwise labels are refined based on MRF spatial regularization.
The parameters of the class-specific Gaussians are then re-estimated.
This iterative process continues until convergence.  We augment this segmentation step 
by iterating the results with application of N4 bias correction [@Tustison:2010ac].
\textcolor{blue}{Unlike other methods that rely solely on intensity distributions, thereby 
discarding spatial information} (e.g., K-means variants [@Kirby:2012aa;@Zha:2016aa] and 
histogram rescaling and thresholding [@He:2014aa]), our technique employs
both spatial and intensity information for probabilistic classification.

Because of our dual structural/functional acquisition protocol [@Qing:2015aa], we also 
previously formulated a joint label fusion (JLF)-based framework [@wang2013] for segmenting
the left and right lungs in proton MRI as well as estimating the lobar volumes 
[@Tustison:2016aa].  This permits us to first identify the lung mask in the proton
MRI.  This information is transferred to the space of the corresponding ventilation MR
image via image registration.  The JLF method relies on a set of atlases (proton MRI 
plus lung labels) which is spatially 
normalized to an unlabeled image where a weighted consensus of the normalized images 
and labels is used to determine each voxel label.  Although the method yields high quality 
results which are fully automated, one of the drawbacks is the time and computational 
resources required to perform the image registration for each member of the atlas set 
and the subsequent voxelwise label consensus estimation.   

Note that we have provided self-contained examples for both of these segmentation
algorithms using ANTs tools:  lung and lobe estimation [@lungLobeEstimation] and
lung ventilation [@lungVentilationSegmentation].  However, given the previously 
outlined benefits of deep learning approaches to these same applications, we expect
that adoption by other groups will be greatly facilitated by the proposed algorithms
described below.

### Preprocessing

\begin{figure}
\centering
\includegraphics[width=\textwidth]{./Figures/N4Denoised.pdf}
\caption{Side-by-side image comparison showing the effects of preprocessing on the proton (top) 
and ventilation (bottom) MRI. (a) 
Uncorrected image showing MR field inhomogeneity and noise. (b) Corresponding corrected 
image in which the bias effects have been ameliorated.}
\label{fig:n4denoised}
\end{figure}

Because of the low-frequency imaging artifacts introduced by confounds such as 
radiofrequency coil inhomogeneity, we perform a retrospective bias 
correction on both proton and ventilation images using the N4 algorithm [@tustison2010].
These are included in our previously proposed ventilation [@Tustison:2011aa]
and structural [@Tustison:2016aa] segmentation frameworks.  Since the initial 
release of these pipelines we have also adopted an adaptive, patch-based
denoising algorithm specific to MR [@Manjon:2010aa] which we have reimplemented 
in the ANTs toolkit.  The effects of these data cleaning techniques on both 
the proton images and ventilation images are shown in Figure \ref{fig:n4denoised}.

### U-net architecture for structural/functional lung segmentation

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{./Figures/Unet.png}
\caption{The modified U-net architecture for both structural and functional 
lung segmentation (although certain parameters, specifically the
number of filters per convolution layer, are specific to the functional
case).  Network layers are represented as boxes with arrows 
designating connections between layers.  The main parameter value for each layer
is provided above the corresponding box.  Each layer 
of the descending (or ``encoding'') branch of the network is characterized
by two convolutional layers.  Modification of 
the original architecture includes an intermediate dropout layer 
for regularization (dropout rate = 0.2).  A max pooling operation 
 produces the feature map for the next series.  The ascending
(or ``decoding'') branch is similarly characterized.  A convolutional transpose
operation is used to upsample the feature map following a convolution $\rightarrow$
dropout $\rightarrow$ convolution layer series until the final convolutional
operation which yields the segmentation probability maps.
}
\label{fig:unet}
\end{figure}

The U-net architecture was introduced in [@Ronneberger:2015aa] which extended the fully 
convolutional neural network (FCN) approach introduced by Long, Shelhamer, and Darrel 
[@Shelhamer:2017aa].  U-net augments the "encoding path" (see left side of Figure 
\ref{fig:unet}) common to such architectures as VGG and FCN with a symmetric decoding
path where the corresponding encoding/decoding layers are linked via skip paths for
enhanced feature detection.  The nomenclature reflects the descending/ascending aspect
of its architecture.  Each series in both encoding and decoding branches is 
characterized  \textcolor{blue}{by an optional dropout layer in between two convolutional layers}.  This 
latter modification from the original is meant to provide additional regularization for 
over-fitting prevention [@Srivastava2014].  Output consists of a segmentation probability
image for each label from which a segmentation map can be generated.  

We used the U-net architecture to build separate models for segmenting both structural and functional lung 
images.  For cases where dual acquisition provides both images, we use the structural 
images to provide a mask for segmentation of the ventilation image.  We used an open-source
implementation written by our group and provided with the ANTsRNet R package [@antsrnet] which
is described in greater detail below.  We also implemented a multi-label dice coefficient
loss function along with specific batch generators for generating augmented image data on the
fly.  

### Template-based data augmentation

The need for large training data sets is a well-known limitation of deep learning 
algorithms.  Whereas the architectures developed for such tasks as the ImageNet 
competition have access to millions of annotated images for training, such data availability
is atypical in medical imaging.  In order to achieve
data set sizes necessary for learning functional models, various data augmentation 
strategies have been employed [@Taylor:2017aa]. These include application of intensity 
transformations, such as brightening and enhanced contrast.  They might also include 
spatial transformations such as arbitrary rotations, translations, and even simulated
elastic deformations.  Such transformations might not be ideal if they do not represent
shape variation \textcolor{blue}{within the typical range exhibited by the} population under study.

We propose a template-based data augmentation approach whereby image data sampled from the population
is used to construct a representative template that is optimal in terms of shape and/or
intensity [@Avants:2010aa].  In addition to the representative template, this template-building 
process yields the transformations to/from each individual image to the template space.
\textcolor{blue}{
This permits a propagation of individual training data to the space of every other
training data as illustrated in Figure \ref{fig:augmentation}.  Specifically, the
template building process produces an invertible, deformable mapping for each
subject.  For the $k^{th}$ subject, $S_k$, and template, $T$, this mapping is 
denoted as $\phi_k$ with inverse given by $\phi^{-1}_k$.  During model 
training, each new augmentated data instance, $S_{new}$, is produced by randomly selecting a 
source subject and target subject, and mapping $S_{source}$ to the space of $S_{target}$
according to}
\begin{align}
  S_{new} = S_{source}\left( \phi^{-1}_{target} \left( \phi_{source} \right) \right).
\end{align}
\textcolor{blue}{Note that each $S$ comprises all channel images and corresponding segmentation images.}
In the simplest case, the training data is used to construct the template and then each 
individual training image and corresponding labels are propagated to the space of every other 
image.  In this way, a training data set of size $N$ can be expanded to a data set of 
size $N^2$.  A slight variation to this would be to 
build a template from $M$ data sets 
(where $M > N$).  Transformations between the training data and the template are then
used to propagate the training data to the spaces of the individual members of the 
template-generating data for an augmented data set size of $M \times N$. 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{./Figures/DataAugmentation.pdf}
\caption{
\textcolor{blue}{Template-based data augmentation for the proton (left) and
ventilation (right) U-net model generation.  For both cases, a template is
created, or selected, to generate the transforms
to and from the template.  The 
derived deformable, invertible transform for the $k^{th}$ subject, $S_k$, to the
template, $T$, is   
denoted by $\phi_k: S_k \leftrightsquigarrow T$.  These subject-specific mappings are 
used during model training (but not the template itself).
Data augmentation occurs by randomly choosing a reference
subject and a target subject during batch processing.  In the illustration above, the sample mapping of 
Subject 1 to the space of Subject 2, represented by the green curved arrow,  
is defined as $\phi^{-1}_2\left( \phi_1 \right)$.}
}
\label{fig:augmentation}
\end{figure}

\textcolor{blue}{
Since U-net model generation is completely separate for the proton and ventilation
data, template-based data augmentation is also isolated between the two protocols.
The proton template is created from the right/left segmentation images of the training data 
denoted by the red/green labels which have voxel values of `1' and `2', respectively,
in Figure \ref{fig:augmentation}.  The resulting 
template located in the center of the left panel is an average of all the transformed
label images.  This whole lung approach avoids the possible lack of internal correspondence while 
generating plausible global shape variations when mapping between individual training data.
We used 60 proton MR images thus permitting 60$^2$ = 3600 possible deformable shapes 
which can be further augmented by more conventional strategies (e.g., brightness
transformations, translations, etc.).
In contrast, the ventilation template is
created directly from the training ventilation images resulting in the grayscale 
template in the center of the right panel of Figure \ref{fig:augmentation}.  An 
alternative strategy for ventilation template creation could have employed relabeling the 
ventilation masks to a single value and aligning the lung boundaries, somewhat similar to 
the proton protocol.  This approach would be necessary for severe lung disease where major
portions of the lung are absent in the ventilation image.    
}

### ANTsRNet

In addition to the contributions previously described, we also introduce ANTsRNet [@antsrnet] to the 
research community which not only contains the software to perform the operations specific 
to structural and functional lung image segmentation but also performs a host of other 
deep learning tasks wrapped in a thoroughly documented and well-written R package.
The recent interest in deep learning techniques and the associated successes with respect to a 
variety of applications \textcolor{blue}{have} motivated adoption of such techniques.
Basic image operations such as classification, object identification, 
segmentation, as well as more focused techniques, \textcolor{blue}{such as predictive image registration} [@Yang:2017aa],
 have significant potential for facilitating 
basic medical research.  ANTsRNet is built using the Keras neural network 
library (available through R) and is highly integrated with the ANTsR package, the R interface 
of the ANTs toolkit.  Consistent with our other software offerings, ongoing development is 
currently carried out on GitHub using a well-commented coding style, thorough documentation, 
and self-contained working examples [@antsrnet].  

<!--
It should be noted that various implementations of different deep learning 
architectures exist and are largely available to the public.  However, we feel 
that ANTsRNet fills an unmet need.  Based on our own search, many publicly 
available implementations, while functional, are not developed with large-scale distribution 
and application as end goals.  There is little, if any, coding consistency between the 
various implementations, leading to non-standardized APIs and difficulties in code
navigation for debugging and/or didactic reasons.  In addition, the vast majority employ the
Python language which is understandable given its widespread usage by data scientists.
However, this work makes these powerful new developments available through a major platform 
heavily used by statisticians and data scientists.
In addition, the R-based interface to the ANTs toolkit allows for preprocessing and data
augmentation strategies specific to medical imaging.  
-->

Several architectures have been implemented for both 2-D and 3-D images spanning the broad
application areas of image classification, object detection, and image segmentation 
(cf. Table \ref{table:antsrnet}). 
It should be noted that most reporting in the literature has dealt exclusively with 2-D 
implementations.  This is understandable due to memory and computational speed constraints
limiting practical 3-D applications on current hardware.  However, given the importance that
3-D data has for medical imaging and the rapid progress in hardware, we  feel it worth
the investment in implementing corresponding 3-D architectures.  Each 
architecture is accompanied by one or more self-contained examples for testing and illustrative
purposes.  In addition, we have made novel data augmentation strategies available to the user 
and illustrated them with Keras-specific batch generators.    

\input{antsrnetTable.tex}

---
nocite: |
  @Ronneberger:2015, @Milletari:2016, @Krizhevsky:2012, @Simonyan:2014, @Szegedy:2015, @He:2015, @Xie:2016, @Huang:2016, @Liu:2015, @Dong:2016aa}
...

### Processing specifics

205 proton MR images each with left/right lung segmentations and 73 ventilation MR images 
with masks were used for the separate U-net model training.  These images were denoised and 
bias corrected offline (as described above) and required < 1 minute for both steps per image 
using single-threading although both preprocessing steps are multi-threading capable.
An R script was used to read in the images and segmentations (available in
our GitHub \textcolor{blue}{repository} [@deepventnet]), create the model, designate model parameters, and initialize 
the batch generator.  

For the proton data we built a 3-D U-net model to take advantage of the characteristic
3-D shape of the lungs.  This limited the possible batch size as our GPU (Titan Xp) is 
limited to 12 GB although this can be revisited in the future with additional computational
resources.  \textcolor{blue}{Transforms derived from the template-building process described
previously were passed to the batch generator where reference and source subjects were 
randomly assigned.  During each iteration, these random pairings were used to create the
augmented data according to Equation (1).  These
data are publicly available for download at https://doi.org/10.6084/m9.figshare.4964915.v1.}

\textcolor{blue}{
The U-net ventilation model was generated from 73 ventilation MRI.  The smaller
data set size was a result of data pruning to ensure class balance.  
Even though the functional images are processed as 3-D volumes and a 3-D
ventilation template is created for the template-based data augmentation, the generated 
U-net model is 2-D.  This is due to lack of any discernible anatomical signatures available
for learning.  This also makes model generation much faster.  The basic processing strategy
is that any ventilation image to be segmented will be processed on a slice-by-slice basis
where each slice is segmented using the 2-D model.
For data augmentation, the full 3-D transforms are supplied to the 
batch generator.  At each iteration, as set of generated 3-D augmented images 
are created on the fly based on Equation (1) and then a subset of slices is 
randomly selected for each image until the batch set is complete.  
For this work we randomly sampled slices
in the coronal direction using a specified sampling rate ($=$ 0.5).
It should
be noted that a whole lung mask is assumed to exist and supplied as an additional channel for 
processing.  Prediction on the evaluation data is performed slice-by-slice
and then collated into probability volumetric 
images.}

\textcolor{blue}{The 3-D U-net structural model and 2-D U-net functional model that were created,
as described above and included with the GitHub repository, can then be used for future processing.
Using these models, the basic deep learning workflow is as follows:
\begin{itemize}
  \item Assume simultaneous structural and functional image acquisition.
  \item Generate left/right lung mask from the structural image using corresponding 3-D U-net structural model.
  \item Convert left/right lung mask to a single label.
  \item Process ventilation image slice-by-slice using the 2-D U-net functional model using the 
        ventilation image and single label structural lung mask as input.
\end{itemize}
Note that this workflow is not absolutely necessary.  For example, one could use the 2-D functional
model if one provides the input lung masks.
}

Image size was not identical across both image cohorts so we settled on a common resampled image size of 
$128 \times 128 \times 64$ for the proton images and $128 \times 128$ for the ventilation images.
Resampling of each image and segmentation was handled internally by the batch generator 
after transformation to the reference image using ANTsR functions [@antsr].   Additionally, 
during data augmentation for proton model optimization, a digital "coin flip" was used to
randomly vary the intensity profile of the warped proton images between their original profiles and 
the intensity profile of the randomly selected reference image.  The latter intensity transformation 
is the histogram matching algorithm of Nyul et al. [@Nyul:2000aa] implemented in the Insight Toolkit.
Specific parameters for the U-net architecture for both models are as follows (3-D parameters are 
included in parentheses):

* Adam optimization:  
    * proton model learning rate = 0.00001
    * ventilation model learning rate = 0.0001
* Number of epochs:  150
* Training/validation data split:  80/20
* Convolution layers
    * kernel size:  $5 \times 5 (\times 5)$
    * activation: rectified linear units (ReLU) [@Nair2010]
    * number of filters:  doubled at every layer starting with $N = 16$ (proton) and $N = 32$ (ventilation)
* Dropout layers
    * rate: $0.2$
* Max pooling layers
    * size: $2 \times 2 (\times 2)$
    * stride length:  $2 \times 2 (\times 2)$
* Upsampling/transposed convolution (i.e., deconvolution) layers
    * kernel size:  $5 \times 5 (\times 5)$
    * stride length:  $2 \times 2 (\times 2)$
    * activation: rectified linear units (ReLU) [@Nair2010]

Training took approximately 10 hours for both models.  After model construction, prediction 
per image (after preprocessing) takes < 1 second per image.  Both model construction 
and prediction utilized a NVIDIA Titan Xp GPU.  
