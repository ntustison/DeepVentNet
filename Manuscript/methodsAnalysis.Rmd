
## _Image processing and analysis_

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{./Figures/workflow.pdf}
\caption{Illustration of the proposed workflow.  Training the U-net models for both proton and 
ventilation imaging includes template-based data augmentation.  This offline training is computationally 
intensive but is only performed once.   Subsequent individual subject preprocessing includes
MR denoising and bias correction.  The proton mask determined from the proton U-net model 
is included as a separate channel (in deep learning software parlance) for ventilation image processing.
}
\label{fig:workflow}
\end{figure}

We first review our previous contributions to the segmentation of proton and 
hyperpolarized gas MR images [@Tustison:2011aa;@Tustison:2016aa] as we use 
these previously described techniques for evaluative comparison.  We then describe 
the deep learning analogs (including preprocessing) extending earlier work
and discuss the proposed contributions which include:

* convolution neural networks for structural/functional lung
segmentation,

* template-based data augmentation, and

* open-source availability.

An overview of the resulting framework is provided in Figure \ref{fig:workflow}.
The most computationally intensive portion is the offline processing for model
training for both structural and functional imaging.  However, once that is 
complete, individual processing consists of a couple of preprocessing steps
followed by application of the models which has minimal computational requirements.  

### Previous approaches from our group for lung and ventilation-based segmentation

The automated ventilation-based segmentation, described in [@Tustison:2011aa], employs
a Gaussian mixture model with a Markov random field (MRF) spatial prior optimized
via the Expectation-Maximization algorithm.  The resulting software, called 
Atropos, has been used in a number of 
clinical studies (e.g., [@Altes:2016aa;@Altes:2017aa]).  Briefly, the intensity histogram profile
of the ventilation image is modeled using Gaussian functions with optimizable parameters 
(i.e., mean, standard deviation, and normalization factor) designed to model the intensities 
of the individual ventilation classes.  At each iteration
the resulting estimated voxelwise labels are refined based an MRF spatial regularization.
The parameters of the class-specific Gaussians are then re-estimated.
This iterative process continues until convergence.  We augment this segmentation step 
by iterating the results with application of N4 bias correction [@Tustison:2010ac].  Unlike other 
methods which rely solely on intensity distributions which discards spatial information,
(e.g., K-means variants [@Kirby:2012aa;@Zha:2016aa] and 
histogram rescaling and thresholding [@He:2014aa]),  our technique employs
both spatial and intensity information for probabilistic classification.

Because of our dual structural/functional acquisition protocol [@Qing:2015aa], we also 
previously formulated a joint label fusion (JLF)-based framework [@wang2013] for segmenting
the left and right lungs in proton MRI as well as estimating the lobar volumes 
[@Tustison:2016aa].  This permits us to first identify the lung mask in the proton
MRI.  This information is transferred to the space of the corresponding ventilation MR
image via image registration.  The JLF method relies on a set of atlases (proton MRI 
plus lung labels) which is spatially 
normalized to an unlabeled image where a weighted consensus of the normalized images 
and labels is used to determine each voxel label.  Although the method yields high quality 
results which are fully automated, one of the drawbacks is the time and computational 
resources required to perform the image registration for each member of the atlas set 
and the subsequent voxelwise label consensus estimation.   

Note that we have provided self-contained examples for both of these segmentation
algorithms using ANTs tools:  lung and lobe estimation [@lungLobeEstimation] and
lung ventilation [@lungVentilationSegmentation].  However, given the previously 
outlined benefits of deep learning approaches to these same applications, we expect
that adoption by other groups will be greatly facilitated by the proposed algorithms
described below.

### Preprocessing

\begin{figure}
\centering
\includegraphics[width=\textwidth]{./Figures/N4Denoised.pdf}
\caption{Side-by-side image comparison showing the effects of preprocessing on the proton (top) 
and ventilation (bottom) MRI. (a) 
Uncorrected image showing MR field inhomogeneity and noise. (b) Corresponding corrected 
image in which the bias effects have been ameliorated.}
\label{fig:n4denoised}
\end{figure}

Because of the low-frequency imaging artifacts introduced by confounds such as 
radiofrequency coil inhomogeneity, we perform a retrospective bias 
correction on both proton and ventilation images using the N4 algorithm [@tustison2010].
These are included in our previously proposed ventilation [@Tustison:2011aa]
and structural [@Tustison:2016aa] segmentation frameworks.  Since the initial 
release of these pipelines we have also adopted an adaptive, patch-based
denoising algorithm specific to MR [@Manjon:2010aa] which we have reimplemented 
in the ANTs toolkit.  The effects of these data cleaning techniques on both 
the proton images and ventilation images are shown in Figure \ref{fig:n4denoised}.

### U-net architecture for structural/functional lung segmentation

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{./Figures/Unet.png}
\caption{The modified U-net architecture for both structural and functional 
lung segmentation (although certain parameters, specifically the
number of filters per convolution layer, are specific to the functional
case).  Network layers are represented as boxes with arrows 
designating connections between layers.  The main parameter value for each layer
is provided above the corresponding box.  Each layer 
of the descending (or ``encoding'') branch of the network is characterized
by two convolutional layers.  Modification of 
the original architecture includes an intermediate dropout layer 
for regularization (dropout rate = 0.2).  A max pooling operation 
 produces the feature map for the next series.  The ascending
(or ``decoding'') branch is similarly characterized.  A convolutional transpose
operation is used to upsample the feature map following a convolution $\rightarrow$
dropout $\rightarrow$ convolution layer series until the final convolutional
operation which yields the segmentation probability maps.
}
\label{fig:unet}
\end{figure}


Deep learning, a term denoting neural network architectures with 
multiple hidden layers, has seen recent renewed research development and application.  In the field of 
image analysis and computer vision, deep learning with convolution neural networks (CNNs)
has been particularly prominent in recent years due, in large part, to the annual ImageNet Large Scale Visual 
Recognition Challenge [@Russakovsky:2015aa].
Specifically, one of the participating groups in the 2012 ImageNet challenge was the earliest 
adopter of CNNs.  The resulting architecture, colloquially known as "AlexNet" [@AlexNet],
surpassed any approach that had been proposed previously and laid the groundwork for
future CNN-based architectures for image classification such as VGG [@Simonyan:2014] and
GoogLeNet [@Szegedy:2015].  The recent successes of CNNs are historically rooted in the 
pioneering work of LeCun et al. [@LeCun1998] and Fukushima [@Fukushima:1980aa] and others
which drew inspiration from earlier work on the complex arrangement of cells within 
the feline visual cortex [@HUBEL:1962aa].  CNNs are characterized by common components (i.e.,
convolution, pooling, and activation functions) which can be put together in various 
arrangements to perform such tasks as image classification and voxelwise segmentation.

The U-net architecture was introduced in [@Ronneberger:2015aa] which extended the fully 
convolutional neural network (FCN) approach introduced by Long, Shelhamer, and Darrel 
[@Shelhamer:2017aa].  U-net augments the "encoding path" (see left side of Figure 
\ref{fig:unet}) common to such architectures as VGG and FCN with a symmetric decoding
path where the corresponding encoding/decoding layers are linked via skip paths for
enhanced feature detection.  The nomenclature reflects the descending/ascending aspect
of its architecture.  Each series in both encoding and decoding branches is 
characterized by two convolutional layers sandwiching an optional dropout layer.  This 
latter modification from the original is meant to provide additional regularization for 
over-fitting prevention [@Srivastava2014].  Output consists of a segmentation probability
image for each label from which a segmentation map can be generated.  

We used the U-net architecture to build separate models for segmenting both structural and functional lung 
images.  For cases where dual acquisition provides both images, we use the structural 
images to provide a mask for segmentation of the ventilation image.  We used an open-source
implementation written by our group and provided with the ANTsRNet R package [@antsrnet] which
is described in greater detail below.  We also implemented a multi-label dice coefficient
loss function along with specific batch generators for generating augmented image data on the
fly.  

### Template-based data augmentation

The need for large training data sets is a well-known limitation of deep learning 
algorithms.  Whereas the architectures developed for such tasks as the ImageNet 
competition have access to millions of annotated images for training, such data availability
is atypical in medical imaging.  In order to achieve
data set sizes necessary for learning functional models, various data augmentation 
strategies have been employed [@Taylor:2017aa]. These include application of intensity 
transformations, such as brightening and enhanced contrast.  They might also include 
spatial transformations such as arbitrary rotations, translations, and even simulated
elastic deformations.  Such transformations might not be ideal if they do not represent
shape variation within the range within the population under study.

We propose a template-based data augmentation approach whereby image data sampled from the population
is used to construct a representative template that is optimal in terms of shape and/or
intensity [@Avants:2010aa].  In addition to the representative template, this template-building 
process yields the transformations to/from each individual image to the template space.
This permits a propagation of the training data to the space of each individual image. In 
the simplest case, the training data is used to construct the template and then each 
individual training image and corresponding labels are propagated to the space of every other 
image.  In this way, a training data set of size $N$ can be expanded to a data set of 
size $N^2$ (see Figure \ref{fig:augmentation}).  A slight variation to this would be to 
build a template from $M$ data sets 
(where $M > N$).  Transformations between the training data and the template are then
used to propagate the training data to the spaces of the individual members of the 
template-generating data for an augmented data set size of $M \times N$. 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{./Figures/DataAugmentation.pdf}
\caption{We introduce a novel data augmentation strategy for medical images using 
ANTs-based template construction.  Shown here is proton lung segmentation example where 
a template is created from the training data segmentation images where the 
foreground designates the left and right lungs.  This avoids the lack of 
internal correspondence while generating plausible global shape variations 
when mapping between individual training data.  We used 60+ images to 
create such a template permitting 60$^2$ = 3600 possible deformable shapes 
which can be further augmented by more conventional strategies (e.g., brightness
transformations, translations, etc.).}
\label{fig:augmentation}
\end{figure}

### ANTsRNet

In addition to the contributions previously described, we also introduce ANTsRNet [@antsrnet] to the 
research community which not only contains the software to perform the operations specific 
to structural and functional lung image segmentation but also performs a host of other 
deep learning tasks wrapped in a thoroughly documented and well-written R package.
The recent interest in deep learning techniques and the associated successes with respect to a 
variety of applications has motivated adoption of such techniques.
Basic image operations such as classification, object identification, and 
segmentation (as well as more focused techniques) have significant potential for facilitating 
basic medical research.  ANTsRNet is built using the Keras neural network 
library (available through R) and is highly integrated with the ANTsR package, the R interface 
of the ANTs toolkit.  Consistent with our other software offerings, ongoing development is 
currently carried out on GitHub using a well-commented coding style, thorough documentation, 
and self-contained working examples [@antsrnet].  

It should be noted that various implementations of different deep learning 
architectures exist and are largely available to the public.  However, we feel 
that ANTsRNet fills an unmet need.  Based on our own search, many publicly 
available implementations, while functional, are not developed with large-scale distribution 
and application as end goals.  There is little, if any, coding consistency between the 
various implementations leading to non-standardized APIs and difficulties in code
navigation for debugging and/or didactic reasons.  In addition, the vast majority employ the
Python language which is understandable given its widespread usage by data scientists.
However, this work makes these powerful new developments available through a major platform 
heavily used by statisticians and data scientists.
In addition, the R-based interface to the ANTs toolkit allows for preprocessing and data
augmentation strategies specific to medical imaging.  

Several architectures have been implemented for both 2-D and 3-D images spanning the broad
application areas of image classification, object detection, and image segmentation 
(cf. Table \ref{table:antsrnet}). 
It should be noted that most reporting in the literature has dealt exclusively with 2-D 
implementations.  This is understandable due to memory and computational speed constraints
limiting practical 3-D applications on current hardware.  However, given the importance that
3-D data has for medical imaging and the rapid progress in hardware, we  feel it worth
the investment in implementing corresponding 3-D architectures.  Each 
architecture is accompanied by one or more self-contained examples for testing and illustrative
purposes.  In addition, we have made novel data augmentation strategies available to the user 
and illustrated them with Keras-specific batch generators.    

\input{antsrnetTable.tex}

---
nocite: |
  @Ronneberger:2015, @Milletari:2016, @Krizhevsky:2012, @Simonyan:2014, @Szegedy:2015, @He:2015, @Xie:2016, @Huang:2016, @Liu:2015}
...

### Processing specifics

205 proton MR images each with left/right lung segmentations and 73 ventilation MR images 
with masks were used for the separate U-net model training.  These images were denoised and 
bias corrected offline (as described above) and required < 1 minute for both steps per image 
using single-threading although both preprocessing steps are multi-threading capable.
An R script was used to read in the images and segmentations (available in
our GitHub repo [@deepventnet]), create the model, designate model parameters, and initialize 
the batch generator.  

For the proton data we built a 3-D U-net model to take advantage of the characteristic
3-D shape of the lungs.  This limited the possible batch size as our GPU (Titan Xp) is 
limited to 12 GB although this can be revisited in the future with additional computational
resources.  We built a 2-D U-net model for the ventilation images as the functional image
segmentation does not take advantage of obvious anatomical factors.

Image size was not identical across both image cohorts so we settled on a common resampled image size of 
$128 \times 128 \times 64$ for the proton images and $128 \times 128$ for the ventilation images.
Resampling of each image and segmentation was handled internally by the batch generator 
after transformation to the reference image using ANTsR functions [@antsr].   Additionally, 
during data augmentation for proton model optimization, a digital "coin flip" was used to
randomly vary the intensity profile of the warped proton images between their original profiles and 
the intensity profile of the randomly selected reference image.  The latter intensity transformation 
is the histogram matching algorithm of Nyul et al. [@Nyul:2000aa] implemented in the Insight Toolkit.
Specific parameters for the U-net architecture for both models are as follows (3-D parameters are 
included in parentheses):

* Adam optimization:  
    * proton model learning rate = 0.00001
    * ventilation model learning rate = 0.0001
* Number of epochs:  150
* Training/validation data split:  80/20
* Convolution layers
    * kernel size:  $5 \times 5 (\times 5)$
    * activation: rectified linear units (ReLU) [@Nair2010]
    * number of filters:  doubled at every layer starting with $N = 16$ (proton) and $N = 32$ (ventilation)
* Dropout layers
    * rate: $0.2$
* Max pooling layers
    * size: $2 \times 2 (\times 2)$
    * stride length:  $2 \times 2 (\times 2)$
* Upsampling/transposed convolution (i.e., deconvolution) layers
    * kernel size:  $5 \times 5 (\times 5)$
    * stride length:  $2 \times 2 (\times 2)$
    * activation: rectified linear units (ReLU) [@Nair2010]

Training took approximately 10 hours for both models.  After model construction, prediction 
per image (after preprocessing) takes < 1 second per image.  Both model construction 
and prediction utilized a NVIDIA Titan Xp GPU.  
